import requests
from bs4 import BeautifulSoup
import time

class SmartCrawler:
    def __init__(self):
        # ìºì‹±: {key: (data, timestamp)}
        self.cache = {}
        self.TTL = 300 # 5ë¶„ (300ì´ˆ)

    def _get_from_cache(self, key):
        if key in self.cache:
            data, timestamp = self.cache[key]
            if time.time() - timestamp < self.TTL:
                return data
        return None

    def _set_cache(self, key, data):
        self.cache[key] = (data, time.time())

    def get_weather(self):
        cached = self._get_from_cache("weather")
        if cached: return cached
        
        try:
            # ë„¤ì´ë²„ ë‚ ì”¨ (ì„œìš¸)
            url = "https://search.naver.com/search.naver?query=ì„œìš¸ë‚ ì”¨"
            res = requests.get(url)
            soup = BeautifulSoup(res.content, 'html.parser')
            
            # êµ¬ì¡°ê°€ ìì£¼ ë°”ë€Œë¯€ë¡œ ì˜ˆì™¸ì²˜ë¦¬ í•„ìˆ˜
            # í˜„ì¬ ê¸°ì˜¨
            temp = soup.find('div', {'class': 'temperature_text'}).text.strip().replace("í˜„ì¬ ì˜¨ë„", "").strip()
            # ë‚ ì”¨ ìƒíƒœ (ë§‘ìŒ, íë¦¼ ë“±)
            status = soup.find('span', {'class': 'weather before_slash'}).text.strip()
            # ë¯¸ì„¸ë¨¼ì§€?
            
            result = f"[ì„œìš¸ ë‚ ì”¨]\nê¸°ì˜¨: {temp}\nìƒíƒœ: {status}"
            self._set_cache("weather", result)
            return result
        except Exception as e:
            return f"ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\n({e})"

    def get_news(self, category):
        cache_key = f"news_{category}"
        cached = self._get_from_cache(cache_key)
        if cached: return cached
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ (ì„¹ì…˜ë³„ URL ë³€ê²½ ëŒ€ì‘)
        cat_map = {"ê²½ì œ": "101", "ê³¼í•™": "105", "ì„¸ê³„": "104"}
        sid1 = cat_map.get(category, "101")
        
        try:
            # ìƒˆë¡œìš´ ë‰´ìŠ¤ ì„¹ì…˜ URL êµ¬ì¡°
            url = f"https://news.naver.com/section/{sid1}"
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0'}
            res = requests.get(url, headers=headers)
            soup = BeautifulSoup(res.content, 'html.parser')
            
            headlines = []
            
            # Selector: sa_text_title
            items = soup.select('.sa_text_title')
            
            seen = set()
            count = 0
            for item in items:
                title = item.text.strip()
                if title not in seen:
                    headlines.append(f"â€¢ {title}")
                    seen.add(title)
                    count += 1
                if count >= 3:
                    break
                
            if not headlines:
                items = soup.select('.cluster_text_headline')
                for item in items[:3]:
                    headlines.append(f"â€¢ {item.text.strip()}")

            if not headlines:
                return f"[{category} ë‰´ìŠ¤] ê°€ì ¸ì˜¬ ë‰´ìŠ¤ê°€ ì—†ì–´ìš”."

            result = f"[{category} ë‰´ìŠ¤] (ì†ë³´)\n" + "\n".join(headlines)
            self._set_cache(cache_key, result)
            return result
        except Exception as e:
            return f"ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\n({e})"

    def get_exchange_rate(self):
        """í™˜ìœ¨ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (USD, JPY)"""
        cache_key = "exchange"
        cached = self._get_from_cache(cache_key)
        if cached: return cached
        
        try:
            # ë„¤ì´ë²„ ê¸ˆìœµ í™˜ìœ¨ í˜ì´ì§€
            url = "https://finance.naver.com/marketindex/"
            # User-Agent í—¤ë” ì¶”ê°€í•˜ì—¬ ìš”ì²­ (ê¸°ì¡´ get_newsì—ì„œ ì‚¬ìš©ëœ í—¤ë” ì¬í™œìš©)
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0'}
            response = requests.get(url, headers=headers, timeout=5)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # USD/KRW
            usd_element = soup.select_one('#exchangeList > li.on > a.head.usd > div > span.value')
            usd = usd_element.text.strip() if usd_element else "N/A"
            
            # JPY/KRW (100ì—” ê¸°ì¤€)
            # ê¸°ì¡´ selector: '#exchangeList > li.on > a.head.jpy > div > span.value'
            # ë³€ê²½ëœ selector: '#exchangeList > li:nth-child(3) > a.head.jpy > div > span.value'
            jpy_element = soup.select_one('#exchangeList > li:nth-child(3) > a.head.jpy > div > span.value')
            jpy = jpy_element.text.strip() if jpy_element else "N/A"
            
            result = f"ğŸ’µ USD: {usd}ì› | ğŸ’´ JPY(100ì—”): {jpy}ì›"
            self._set_cache(cache_key, result)
            return result
        except Exception as e:
            return f"í™˜ìœ¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ ({e})"
